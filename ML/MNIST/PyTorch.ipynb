{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.python-course.eu/neural_network_mnist.php\n",
    "with open('MNIST/mnist_train.csv','r') as f:\n",
    "    lines = f.readlines()\n",
    "train_labels = torch.tensor([int(a.split(',')[0]) for a in lines]).long()\n",
    "train_images = torch.tensor([list(map(int,a.split(',')[1:])) for a in lines]).float()\n",
    "\n",
    "with open('MNIST/mnist_test.csv','r') as f:\n",
    "    lines = f.readlines()\n",
    "test_labels = torch.tensor([int(a.split(',')[0]) for a in lines]).long()\n",
    "test_images = torch.tensor([list(map(int,a.split(',')[1:])) for a in lines]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 60000 784\n",
      "10000 10000 784\n"
     ]
    }
   ],
   "source": [
    "print(len(train_labels),len(train_images),len(train_images[0]))\n",
    "print(len(test_labels),len(test_images),len(test_images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  18.,  18.,  18.,\n",
      "        126., 136., 175.,  26., 166., 255., 247., 127.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  30.,  36.,  94., 154.,\n",
      "        170., 253., 253., 253., 253., 253., 225., 172., 253., 242., 195.,  64.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  49.,\n",
      "        238., 253., 253., 253., 253., 253., 253., 253., 253., 251.,  93.,  82.,\n",
      "         82.,  56.,  39.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,  18., 219., 253., 253., 253., 253., 253., 198., 182.,\n",
      "        247., 241.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107., 253.,\n",
      "        253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,  14.,   1., 154., 253.,  90.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0., 139., 253., 190.,   2.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  11.,\n",
      "        190., 253.,  70.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  81., 240., 253.,\n",
      "        253., 119.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,  45., 186., 253., 253., 150.,  27.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,  16.,  93., 252., 253., 187.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0., 249., 253., 249.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,  39., 148., 229., 253., 253., 253., 250., 182.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,  24., 114., 221., 253., 253., 253.,\n",
      "        253., 201.,  78.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  23.,  66., 213., 253.,\n",
      "        253., 253., 253., 198.,  81.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  18., 171.,\n",
      "        219., 253., 253., 253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         55., 172., 226., 253., 253., 253., 253., 244., 133.,  11.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135., 132.,  16.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_images[0])\n",
    "plt.imshow(train_images[0].reshape(28,28))\n",
    "plt.title(str(train_labels[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.9216e-05, 3.9216e-05, 3.9216e-05,  ..., 3.9216e-05, 3.9216e-05,\n",
       "         3.9216e-05],\n",
       "        [3.9216e-05, 3.9216e-05, 3.9216e-05,  ..., 3.9216e-05, 3.9216e-05,\n",
       "         3.9216e-05],\n",
       "        [3.9216e-05, 3.9216e-05, 3.9216e-05,  ..., 3.9216e-05, 3.9216e-05,\n",
       "         3.9216e-05],\n",
       "        ...,\n",
       "        [3.9216e-05, 3.9216e-05, 3.9216e-05,  ..., 3.9216e-05, 3.9216e-05,\n",
       "         3.9216e-05],\n",
       "        [3.9216e-05, 3.9216e-05, 3.9216e-05,  ..., 3.9216e-05, 3.9216e-05,\n",
       "         3.9216e-05],\n",
       "        [3.9216e-05, 3.9216e-05, 3.9216e-05,  ..., 3.9216e-05, 3.9216e-05,\n",
       "         3.9216e-05]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor((train_images.numpy()+.01)/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = torch.tensor((train_images.numpy()+.01)/255.0)\n",
    "test_images = torch.tensor((test_images.numpy()+.01)/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 1.1804e-02, 7.0627e-02, 7.0627e-02, 7.0627e-02,\n",
      "        4.9416e-01, 5.3337e-01, 6.8631e-01, 1.0200e-01, 6.5102e-01, 1.0000e+00,\n",
      "        9.6867e-01, 4.9808e-01, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 1.1769e-01, 1.4122e-01, 3.6867e-01, 6.0396e-01,\n",
      "        6.6671e-01, 9.9220e-01, 9.9220e-01, 9.9220e-01, 9.9220e-01, 9.9220e-01,\n",
      "        8.8239e-01, 6.7455e-01, 9.9220e-01, 9.4906e-01, 7.6475e-01, 2.5102e-01,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 1.9220e-01,\n",
      "        9.3337e-01, 9.9220e-01, 9.9220e-01, 9.9220e-01, 9.9220e-01, 9.9220e-01,\n",
      "        9.9220e-01, 9.9220e-01, 9.9220e-01, 9.8435e-01, 3.6475e-01, 3.2161e-01,\n",
      "        3.2161e-01, 2.1965e-01, 1.5298e-01, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 7.0627e-02, 8.5886e-01, 9.9220e-01,\n",
      "        9.9220e-01, 9.9220e-01, 9.9220e-01, 9.9220e-01, 7.7651e-01, 7.1376e-01,\n",
      "        9.6867e-01, 9.4514e-01, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.1376e-01, 6.1180e-01, 4.1965e-01, 9.9220e-01,\n",
      "        9.9220e-01, 8.0396e-01, 4.3176e-02, 3.9216e-05, 1.6867e-01, 6.0396e-01,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 5.4941e-02, 3.9608e-03, 6.0396e-01, 9.9220e-01, 3.5298e-01,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 5.4514e-01, 9.9220e-01, 7.4514e-01, 7.8824e-03, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 4.3176e-02,\n",
      "        7.4514e-01, 9.9220e-01, 2.7455e-01, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 1.3729e-01, 9.4514e-01,\n",
      "        8.8239e-01, 6.2749e-01, 4.2357e-01, 3.9608e-03, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.1769e-01, 9.4122e-01, 9.9220e-01,\n",
      "        9.9220e-01, 4.6671e-01, 9.8078e-02, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 1.7651e-01, 7.2945e-01, 9.9220e-01, 9.9220e-01,\n",
      "        5.8827e-01, 1.0592e-01, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 6.2784e-02, 3.6475e-01, 9.8827e-01, 9.9220e-01, 7.3337e-01,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 9.7651e-01, 9.9220e-01, 9.7651e-01, 2.5102e-01, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 1.8043e-01, 5.0984e-01, 7.1769e-01, 9.9220e-01,\n",
      "        9.9220e-01, 8.1180e-01, 7.8824e-03, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 1.5298e-01, 5.8043e-01,\n",
      "        8.9808e-01, 9.9220e-01, 9.9220e-01, 9.9220e-01, 9.8043e-01, 7.1376e-01,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        9.4157e-02, 4.4710e-01, 8.6671e-01, 9.9220e-01, 9.9220e-01, 9.9220e-01,\n",
      "        9.9220e-01, 7.8827e-01, 3.0592e-01, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 9.0235e-02, 2.5886e-01, 8.3533e-01, 9.9220e-01,\n",
      "        9.9220e-01, 9.9220e-01, 9.9220e-01, 7.7651e-01, 3.1769e-01, 7.8824e-03,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 7.0627e-02, 6.7063e-01,\n",
      "        8.5886e-01, 9.9220e-01, 9.9220e-01, 9.9220e-01, 9.9220e-01, 7.6475e-01,\n",
      "        3.1376e-01, 3.5333e-02, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        2.1573e-01, 6.7455e-01, 8.8631e-01, 9.9220e-01, 9.9220e-01, 9.9220e-01,\n",
      "        9.9220e-01, 9.5690e-01, 5.2161e-01, 4.3176e-02, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 5.3337e-01, 9.9220e-01,\n",
      "        9.9220e-01, 9.9220e-01, 8.3141e-01, 5.2945e-01, 5.1769e-01, 6.2784e-02,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05,\n",
      "        3.9216e-05, 3.9216e-05, 3.9216e-05, 3.9216e-05])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_images[0])\n",
    "plt.imshow(train_images[0].reshape(28,28))\n",
    "plt.title(str(train_labels[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, samples, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "\n",
    "        # Load data and get label\n",
    "        X = self.samples[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7402\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "# https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "max_epochs = 100\n",
    "\n",
    "# Generators\n",
    "trainset = Dataset(train_images,train_labels)\n",
    "trainloader = data.DataLoader(trainset, **params)\n",
    "\n",
    "testset = Dataset(test_images, test_labels)\n",
    "testloader = data.DataLoader(testset, **params)\n",
    "\n",
    "classes = [str(a) for a in range(10)]\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset \n",
    "batch_size = 64\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch\n",
      "64 784 tensor(0.)\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.FloatTensor torch.FloatTensor\n",
      "Mine\n",
      "64 784 tensor(3.9216e-05)\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.FloatTensor torch.FloatTensor\n",
      "Mine\n",
      "64 784 tensor(3.9216e-05)\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.FloatTensor torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    print('Pytorch')\n",
    "    y = images.reshape(-1, 28*28)\n",
    "    print(len(y),len(y[0]),y[0][0])\n",
    "    print(type(y),type(y[0]))\n",
    "    print(y.type(),y[0].type())\n",
    "    #print(labels,type(labels))\n",
    "    break\n",
    "for i, (images, labels) in enumerate(trainloader):\n",
    "    print('Mine')\n",
    "    print(len(images),len(images[0]),images[0][0])\n",
    "    print(type(images),type(images[0]))\n",
    "    print(images.type(),images[0].type())\n",
    "    #print(labels,type(labels))\n",
    "    break\n",
    "for i, (images, labels) in enumerate(trainloader):\n",
    "    images = images.float()\n",
    "    print('Mine')\n",
    "    print(len(images),len(images[0]),images[0][0])\n",
    "    print(type(images),type(images[0]))\n",
    "    print(images.type(),images[0].type())\n",
    "    #print(labels,type(labels))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch\n",
      "64 tensor(3)\n",
      "torch.LongTensor torch.LongTensor\n",
      "Mine\n",
      "64 tensor(0)\n",
      "torch.LongTensor torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    print('Pytorch')\n",
    "    print(len(labels),labels[0])\n",
    "    print(labels.type(),labels[0].type())\n",
    "    break\n",
    "for i, (images, labels) in enumerate(trainloader):\n",
    "    print('Mine')\n",
    "    print(len(labels),labels[0])\n",
    "    print(labels.type(),labels[0].type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/938], Loss: 0.4004\n",
      "Epoch [1/5], Step [200/938], Loss: 0.3981\n",
      "Epoch [1/5], Step [300/938], Loss: 0.2537\n",
      "Epoch [1/5], Step [400/938], Loss: 0.1740\n",
      "Epoch [1/5], Step [500/938], Loss: 0.2221\n",
      "Epoch [1/5], Step [600/938], Loss: 0.2284\n",
      "Epoch [1/5], Step [700/938], Loss: 0.2539\n",
      "Epoch [1/5], Step [800/938], Loss: 0.1104\n",
      "Epoch [1/5], Step [900/938], Loss: 0.1664\n",
      "Epoch [2/5], Step [100/938], Loss: 0.0989\n",
      "Epoch [2/5], Step [200/938], Loss: 0.0289\n",
      "Epoch [2/5], Step [300/938], Loss: 0.1771\n",
      "Epoch [2/5], Step [400/938], Loss: 0.0537\n",
      "Epoch [2/5], Step [500/938], Loss: 0.0347\n",
      "Epoch [2/5], Step [600/938], Loss: 0.0634\n",
      "Epoch [2/5], Step [700/938], Loss: 0.0600\n",
      "Epoch [2/5], Step [800/938], Loss: 0.0961\n",
      "Epoch [2/5], Step [900/938], Loss: 0.0311\n",
      "Epoch [3/5], Step [100/938], Loss: 0.0745\n",
      "Epoch [3/5], Step [200/938], Loss: 0.0285\n",
      "Epoch [3/5], Step [300/938], Loss: 0.0274\n",
      "Epoch [3/5], Step [400/938], Loss: 0.0307\n",
      "Epoch [3/5], Step [500/938], Loss: 0.0132\n",
      "Epoch [3/5], Step [600/938], Loss: 0.0323\n",
      "Epoch [3/5], Step [700/938], Loss: 0.0291\n",
      "Epoch [3/5], Step [800/938], Loss: 0.0065\n",
      "Epoch [3/5], Step [900/938], Loss: 0.1154\n",
      "Epoch [4/5], Step [100/938], Loss: 0.0056\n",
      "Epoch [4/5], Step [200/938], Loss: 0.0337\n",
      "Epoch [4/5], Step [300/938], Loss: 0.0456\n",
      "Epoch [4/5], Step [400/938], Loss: 0.0338\n",
      "Epoch [4/5], Step [500/938], Loss: 0.0386\n",
      "Epoch [4/5], Step [600/938], Loss: 0.0989\n",
      "Epoch [4/5], Step [700/938], Loss: 0.0331\n",
      "Epoch [4/5], Step [800/938], Loss: 0.0246\n",
      "Epoch [4/5], Step [900/938], Loss: 0.0085\n",
      "Epoch [5/5], Step [100/938], Loss: 0.0238\n",
      "Epoch [5/5], Step [200/938], Loss: 0.0447\n",
      "Epoch [5/5], Step [300/938], Loss: 0.0092\n",
      "Epoch [5/5], Step [400/938], Loss: 0.0106\n",
      "Epoch [5/5], Step [500/938], Loss: 0.0739\n",
      "Epoch [5/5], Step [600/938], Loss: 0.0223\n",
      "Epoch [5/5], Step [700/938], Loss: 0.0251\n",
      "Epoch [5/5], Step [800/938], Loss: 0.1073\n",
      "Epoch [5/5], Step [900/938], Loss: 0.0050\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(trainloader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(trainloader):  \n",
    "        # Move tensors to the configured device\n",
    "        #images = images.reshape(-1, 28*28).to(device)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
